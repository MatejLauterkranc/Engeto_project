import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import re

# --- Z√°kladn√≠ konfigurace ---
base_url = "https://www.auto-data.net"
initial_url = base_url + "/en/"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# --- Seznamy pro ukl√°d√°n√≠ dat z jednotliv√Ωch krok≈Ø ---
all_brands = []
all_models = []
all_generations = []
all_engine_variants = []
final_dataset = []

# --- Pomocn√© funkce pro ƒçi≈°tƒõn√≠ dat ---

# Funkce pro ƒçi≈°tƒõn√≠ kl√≠ƒç≈Ø (n√°zv≈Ø sloupc≈Ø)
def clean_spec_key(key_text):
    key_text = key_text.strip()
    key_text = re.sub(r'\s*\(.*?\)', '', key_text).strip()
    key_text = re.sub(r'\s*\d+[\d\s.,]*\s*(Hp|Nm|l|cm3|cu\. in\.|mm|in\.|lb\.-ft\.|km/h|mph|sec|kg|lbs|litres|gallons|seats|doors|cylinders|Hz|m)\s*$', '', key_text, flags=re.IGNORECASE).strip()
    key_text = re.sub(r'\s*(type|configuration|system|architecture|specs|size|arrangement|valvetrain)\s*$', '', key_text, flags=re.IGNORECASE).strip()
    key_text = re.sub(r'[\s\.,\?!:;]+', ' ', key_text).strip()
    
    if key_text:
        key_text = key_text[0].upper() + key_text[1:]
    return key_text.strip()

# Funkce pro z√≠sk√°n√≠ hodnoty z <td> tagu
def get_value_from_td(td_tag):
    return td_tag.get_text(separator=" ", strip=True)

# --- KROK 1: Extract Brands ---
print(f"\n--- KROK 1: Scrapov√°n√≠ znaƒçek z {initial_url} ---")
try:
    response = requests.get(initial_url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    brands_container = soup.find("div", class_="markite")
    if brands_container:
        brand_links = brands_container.find_all("a", class_="marki_blok")
        print(f"   Nalezeno {len(brand_links)} potenci√°ln√≠ch znaƒçek. Zpracov√°v√°m pouze prvn√≠ (testovac√≠ bƒõh).")
        for i, link in enumerate(brand_links[:1]): # Zpracov√°v√°me pouze prvn√≠ znaƒçku
            brand_name = link.find("strong").text.strip() if link.find("strong") else "N/A"
            relative_url = link.get("href")
            full_brand_url = base_url + relative_url
            if brand_name and brand_name.lower() != "all brands":
                all_brands.append({"BrandName": brand_name, "BrandURL": full_brand_url})
                print(f"   Zpracov√°v√°m znaƒçku: {brand_name} ({i+1}/1)")
    else:
        print("‚ùå Kontejner znaƒçek (<div class='markite'>) nebyl nalezen.")
    print(f"‚úÖ Krok 1 dokonƒçen. Z√≠sk√°no {len(all_brands)} znaƒçek.")
except requests.exceptions.RequestException as e:
    print(f"‚ùå CHYBA p≈ôi stahov√°n√≠ √∫vodn√≠ str√°nky: {e}")
    exit()
except Exception as e:
    print(f"‚ùå NEƒåEKAN√Å CHYBA p≈ôi zpracov√°n√≠ √∫vodn√≠ str√°nky: {e}")
    exit()

# --- KROK 2: Extract Models for each Brand ---
if all_brands:
    print("\n--- KROK 2: Stahov√°n√≠ model≈Ø pro ka≈ædou znaƒçku ---")
    for i, brand_info in enumerate(all_brands): # Zpracov√°v√°me v≈°echny z√≠skan√© znaƒçky z Kroku 1
        brand_name = brand_info["BrandName"]
        brand_url = brand_info["BrandURL"]
        print(f"   Stahuji modely pro znaƒçku: {brand_name} ({i+1}/{len(all_brands)})")
        time.sleep(0.5)
        try:
            brand_response = requests.get(brand_url, headers=headers)
            brand_response.raise_for_status()
            brand_soup = BeautifulSoup(brand_response.text, "html.parser")
            model_links = brand_soup.find_all("a", class_="modeli")
            if model_links:
                print(f"     Nalezeno {len(model_links)} potenci√°ln√≠ch model≈Ø. Zpracov√°v√°m pouze prvn√≠ (testovac√≠ bƒõh).")
                for model_link in model_links[:1]: # Zpracov√°v√°me pouze prvn√≠ model
                    model_name = model_link.find("strong").text.strip() if model_link.find("strong") else "N/A"
                    production_years = model_link.find("div", class_="redcolor").text.strip() if model_link.find("div", class_="redcolor") else "N/A"
                    relative_model_url = model_link.get("href")
                    full_model_url = base_url + relative_model_url
                    all_models.append(
                        {
                            "BrandName": brand_name,
                            "ModelName": model_name,
                            "ProductionYears": production_years,
                            "ModelURL": full_model_url,
                        }
                    )
                    print(f"     Zpracov√°v√°m model: {model_name} (1/1)")
            else:
                print(f"     Nenalezeny ≈æ√°dn√© odkazy na modely na {brand_url}")
        except requests.exceptions.RequestException as e:
            print(f"   ‚ùå CHYBA p≈ôi stahov√°n√≠ str√°nky znaƒçky {brand_name}: {e}")
        except Exception as e:
            print(f"   ‚ùå NEƒåEKAN√Å CHYBA p≈ôi zpracov√°n√≠ str√°nky znaƒçky {brand_name}: {e}")
    print(f"‚úÖ Krok 2 dokonƒçen. Z√≠sk√°no {len(all_models)} model≈Ø.")

# --- KROK 3: Extract Generations for each Model ---
if all_models:
    print("\n--- KROK 3: Stahov√°n√≠ generac√≠ pro ka≈æd√Ω model ---")
    for i, model_info in enumerate(all_models): # Zpracov√°v√°me v≈°echny z√≠skan√© modely z Kroku 2
        brand_name = model_info["BrandName"]
        model_name = model_info["ModelName"]
        model_url = model_info["ModelURL"]
        print(f"   Stahuji generace pro: {brand_name} {model_name} ({i+1}/{len(all_models)})")
        time.sleep(0.5)
        try:
            model_response = requests.get(model_url, headers=headers)
            model_response.raise_for_status()
            model_soup = BeautifulSoup(model_response.text, "html.parser")
            generation_links = model_soup.find_all("a", class_="position")
            if generation_links:
                print(f"     Nalezeno {len(generation_links)} potenci√°ln√≠ch generac√≠. Zpracov√°v√°m pouze prvn√≠ (testovac√≠ bƒõh).")
                for gen_link in generation_links[:1]: # Zpracov√°v√°me pouze prvn√≠ generaci
                    generation_name = gen_link.find("strong", class_="tit").text.strip() if gen_link.find("strong", class_="tit") else "N/A"
                    relative_generation_url = gen_link.get("href")
                    full_generation_url = base_url + relative_generation_url
                    all_generations.append(
                        {
                            "BrandName": brand_name,
                            "ModelName": model_name,
                            "GenerationName": generation_name,
                            "GenerationURL": full_generation_url,
                        }
                    )
                    print(f"     Zpracov√°v√°m generaci: {generation_name} (1/1)")
            else:
                print(f"     Nenalezeny ≈æ√°dn√© odkazy na generace na {model_url}")
        except requests.exceptions.RequestException as e:
            print(f"   ‚ùå CHYBA p≈ôi stahov√°n√≠ str√°nky modelu {brand_name} {model_name}: {e}")
        except Exception as e:
            print(f"   ‚ùå NEƒåEKAN√Å CHYBA p≈ôi zpracov√°n√≠ str√°nky modelu {brand_name} {model_name}: {e}")
    print(f"‚úÖ Krok 3 dokonƒçen. Z√≠sk√°no {len(all_generations)} generac√≠.")

# --- KROK 4: Extract Engine Variants for each Generation ---
if all_generations:
    print("\n--- KROK 4: Stahov√°n√≠ variant motoru pro ka≈ædou generaci ---")
    for i, gen_info in enumerate(all_generations): # Zpracov√°v√°me v≈°echny z√≠skan√© generace z Kroku 3
        brand_name = gen_info["BrandName"]
        model_name = gen_info["ModelName"]
        generation_name = gen_info["GenerationName"]
        generation_url = gen_info["GenerationURL"]
        
        print(f"   Stahuji varianty pro: {brand_name} {model_name} {generation_name} ({i+1}/{len(all_generations)})")
        time.sleep(0.5)
        try:
            gen_response = requests.get(generation_url, headers=headers)
            gen_response.raise_for_status()
            gen_soup = BeautifulSoup(gen_response.text, "html.parser")
            engine_variant_links = gen_soup.find_all("a", href=True)

            if engine_variant_links:
                print(f"     Nalezeno {len(engine_variant_links)} potenci√°ln√≠ch odkaz≈Ø. Nyn√≠ vyhodnocuji ka≈æd√Ω odkaz.")
                
                found_engine_variant = False
                for link in engine_variant_links:
                    strong_tag = link.find("strong")
                    if not strong_tag:
                        continue 

                    title_span = strong_tag.find("span", class_="tit")
                    end_span_text = strong_tag.find("span", class_="end").text.strip() if strong_tag.find("span", class_="end") else 'N/A'
                    
                    if (
                        title_span # Mus√≠ m√≠t n√°zev
                        and link.get("href") # Mus√≠ m√≠t href
                        and link.get("href").endswith(tuple("0123456789")) # Mus√≠ konƒçit ƒç√≠slem (typick√© pro detailn√≠ str√°nky)
                        and "hp-" in link.get("href") # P≈ôid√°me specifickou kontrolu jako 'hp-'
                    ):
                        print(f"     SHODA! Nalezen platn√Ω odkaz na variantu: {link.get('href')} s n√°zvem '{title_span.text.strip()}'")

                        engine_variant_name = title_span.text.strip()
                        engine_production_years = end_span_text 
                        relative_engine_url = link.get("href")
                        full_engine_url = base_url + relative_engine_url

                        all_engine_variants.append(
                            {
                                "BrandName": brand_name,
                                "ModelName": model_name,
                                "GenerationName": generation_name,
                                "EngineVariantName": engine_variant_name,
                                "EngineVariantProductionYears": engine_production_years,
                                "EngineVariantURL": full_engine_url,
                            }
                        )
                        print(f"     Zpracov√°v√°m variantu motoru: {engine_variant_name} (1/1)")
                        found_engine_variant = True
                        break # D≈Øle≈æit√©: Zastav√≠me se po nalezen√≠ prvn√≠ platn√© varianty motoru (pro tento testovac√≠ bƒõh)
                
                if not found_engine_variant:
                    print(f"     Nenalezen ≈æ√°dn√Ω platn√Ω odkaz na variantu motoru odpov√≠daj√≠c√≠ krit√©ri√≠m na {generation_url}")

            else:
                print(f"     Nenalezeny ≈æ√°dn√© odkazy na varianty motoru na {generation_url}")

        except requests.exceptions.RequestException as e:
            print(f"   ‚ùå CHYBA p≈ôi stahov√°n√≠ str√°nky generace {brand_name} {model_name} {generation_name}: {e}")
        except Exception as e:
            print(f"   ‚ùå NEƒåEKAN√Å CHYBA p≈ôi zpracov√°n√≠ str√°nky generace {brand_name} {model_name} {generation_name}: {e}")
    print(f"‚úÖ Krok 4 dokonƒçen. Z√≠sk√°no {len(all_engine_variants)} variant motoru.")
else:
    print("‚ö†Ô∏è  Nebyly z√≠sk√°ny ≈æ√°dn√© generace pro dal≈°√≠ zpracov√°n√≠ variant motoru. Ukonƒçuji.")
    exit()

# --- KROK 5: Download detailed specifications for each engine variant ---
if all_engine_variants:
    print("\n--- KROK 5: Stahov√°n√≠ detailn√≠ch specifikac√≠ pro ka≈ædou variantu motoru ---")
    print("   UPOZORNƒöN√ç: Toto je testovac√≠ bƒõh pro jedin√Ω z√°znam. ≈Ω√°dn√© mezilehl√© CSV soubory nebudou ulo≈æeny.")

    sleep_time_per_spec = 1 # Prodleva mezi stahov√°n√≠m jednotliv√Ωch specifikac√≠

    data_source = all_engine_variants # V produkƒçn√≠m k√≥du byste zde iterovali p≈ôes v≈°echny varianty

    for i, eng_var_info in enumerate(data_source): # Proch√°z√≠me z√≠skan√© varianty motoru
        brand_name = eng_var_info["BrandName"]
        model_name = eng_var_info["ModelName"]
        generation_name = eng_var_info["GenerationName"]
        engine_variant_name = eng_var_info["EngineVariantName"]
        engine_variant_url = eng_var_info["EngineVariantURL"]

        print(f"   ({i+1}/{len(data_source)}) Stahuji specifikace pro: {brand_name} {model_name} {generation_name} - {engine_variant_name}")
        print(f"     URL specifikac√≠: {engine_variant_url}")
        time.sleep(sleep_time_per_spec)

        try:
            current_spec_data = {
                "BrandName": brand_name,
                "ModelName": model_name,
                "GenerationName": generation_name,
                "EngineVariantName": engine_variant_name,
                "EngineVariantURL": engine_variant_url,
                "EngineVariantProductionYears": eng_var_info["EngineVariantProductionYears"],
            }

            # --- POU≈Ω√çV√ÅME POUZE REQUESTS ---
            response = requests.get(engine_variant_url, headers=headers)
            response.raise_for_status() # Vyvol√° v√Ωjimku pro chybov√© stavy HTTP (4xx nebo 5xx)
            spec_html = response.text
            # --- KONEC POU≈ΩIT√ç POUZE REQUESTS ---

            spec_soup = BeautifulSoup(spec_html, "html.parser")

            main_spec_table = spec_soup.find("table", class_="cardetailsout car2")
            
            if not main_spec_table:
                print(f"     ‚ùå CHYBA: Hlavn√≠ tabulka specifikac√≠ (class='cardetailsout car2') nebyla nalezena na str√°nce {engine_variant_url}. P≈ôid√°v√°m pouze z√°kladn√≠ info.")
                final_dataset.append(current_spec_data)
                continue

            spec_table_body = main_spec_table.find("tbody") 

            if not spec_table_body:
                print(f"     ‚ùå CHYBA: tbody nebyl nalezen uvnit≈ô hlavn√≠ tabulky specifikac√≠ na str√°nce {engine_variant_url}. P≈ôid√°v√°m pouze z√°kladn√≠ info.")
                final_dataset.append(current_spec_data)
                continue

            rows = spec_table_body.find_all("tr")
            print(f"     DEBUG: Nalezeno {len(rows)} ≈ô√°dk≈Ø v tbody hlavn√≠ tabulky specifikac√≠.")
            
            for row_idx, row in enumerate(rows):
                # Filtrov√°n√≠ ne≈æ√°douc√≠ch ≈ô√°dk≈Ø: nadpisy sekc√≠ a reklamy
                th_tag = row.find("th")
                if th_tag:
                    # Pokud je to nadpis sekce (nap≈ô. "General information")
                    if th_tag.has_attr('colspan') and th_tag['colspan'] == '2':
                        # print(f"       DEBUG: P≈ôeskakuji ≈ô√°dek s nadpisem sekce: '{th_tag.text.strip()}' v tbody[{row_idx}].")
                        continue
                    # Pokud je to reklama uvnit≈ô <th>
                    if th_tag.find("div", class_="adin"):
                        # print(f"       DEBUG: P≈ôeskakuji ≈ô√°dek s reklamou v <th> v tbody[{row_idx}].")
                        continue
                
                # Zpracov√°n√≠ ≈ô√°dk≈Ø s daty (th a td)
                header = row.find("th")
                value_td = row.find("td") 

                if header and value_td: # Ujist√≠me se, ≈æe oba tagy existuj√≠
                    raw_key = header.text.strip()
                    key = clean_spec_key(raw_key) 
                    val = get_value_from_td(value_td)

                    if "Log in to see" in val: # Pokud je hodnota "Log in to see...", pova≈æujeme ji za pr√°zdnou
                        val = None
                    
                    print(f"       DEBUG: V ≈ô√°dku [{row_idx}]: P≈Øvodn√≠ kl√≠ƒç: '{raw_key}' -> Vyƒçi≈°tƒõn√Ω kl√≠ƒç: '{key}', Extrahovan√° hodnota: '{val}'")

                    current_spec_data[key] = val
                # else: 
                #    print(f"       DEBUG: P≈ôeskakuji ≈ô√°dek bez relevantn√≠ch dat v tbody[{row_idx}]. HTML ≈ô√°dku: {row}")
            
            final_dataset.append(current_spec_data)

        except requests.exceptions.RequestException as e: 
            print(f"   ‚ùå CHYBA stahov√°n√≠ specifikac√≠ pro {engine_variant_url}: {e}")
            final_dataset.append(current_spec_data) # P≈ôid√°me alespo≈à z√°kladn√≠ info v p≈ô√≠padƒõ chyby
        except Exception as e:
            print(f"   ‚ùå NEƒåEKAN√Å CHYBA p≈ôi zpracov√°n√≠ specifikac√≠ pro {engine_variant_url}: {e}")
            final_dataset.append(current_spec_data) # P≈ôid√°me alespo≈à z√°kladn√≠ info v p≈ô√≠padƒõ chyby

# --- Ulo≈æen√≠ z√≠skan√Ωch dat do jedin√©ho CSV/Excel souboru ---
if final_dataset:
    print("\n--- ‚úÖ DOKONƒåENO: Ukl√°d√°n√≠ kompletn√≠ch specifikac√≠ do jedin√©ho souboru ---")
    print(f"   Celkem z√≠skan√Ωch detailn√≠ch specifikac√≠: {len(final_dataset)}")

    df_final = pd.DataFrame(final_dataset)

    output_csv_filename = "full_car_specs.csv"
    df_final.to_csv(output_csv_filename, index=False, encoding="utf-8")
    print(f"   Kompletn√≠ specifikace ulo≈æeny do {output_csv_filename}")

    try:
        output_excel_filename = "full_car_specs.xlsx"
        df_final.to_excel(output_excel_filename, index=False)
        print(f"   Kompletn√≠ specifikace ulo≈æeny do {output_excel_filename}")
    except ImportError:
        print("   ‚ö†Ô∏è  Knihovna 'openpyxl' nen√≠ nainstalov√°na. Nainstalujte ji pro ukl√°d√°n√≠ do Excelu (pip install openpyxl).")

    print("\n   DataFrame s kompletn√≠mi specifikacemi (prvn√≠ch 5 ≈ô√°dk≈Ø):")
    print(df_final.head())
    print(f"   Poƒçet sloupc≈Ø (parametr≈Ø): {len(df_final.columns)}")

else:
    print("‚ùå Nebyly nalezeny ≈æ√°dn√© detailn√≠ specifikace nebo do≈°lo k chyb√°m. V√Ωsledn√Ω soubor nebude vytvo≈ôen.")

print("\n--- üèÅ Skript dokonƒçen ---")